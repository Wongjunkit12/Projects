# Machine Learning using RNNs and Transformers for Text Classification

In this assignment, I focused on applying machine learning concepts to build and train deep neural networks specifically designed for sequential data, including Recurrent Neural Networks (RNNs) and Transformers. The goal was to demonstrate a comprehensive understanding of these architectures by constructing models for a text classification task using a dataset of 2,000 questions categorized into six distinct classes: abbreviation, entity, description, human, location, and numeric.

The project involved several key components: preprocessing the data, implementing various RNN architectures (including simple RNNs, GRUs, and LSTMs), and utilizing Transformers for sequence modeling. I also explored hyperparameter tuning to optimize model performance. By employing techniques such as Word2Vec for text vectorization and attention mechanisms within the RNNs, I aimed to enhance the models' ability to accurately classify questions based on their content.

Through this project, I gained practical experience in handling sequential data with deep learning models. I learned how to effectively implement RNNs and Transformers, understanding their strengths in capturing temporal dependencies in text. Additionally, I developed skills in model evaluation and optimization, which are crucial for improving classification accuracy. This project not only reinforced my theoretical knowledge of machine learning but also equipped me with hands-on skills applicable to real-world natural language processing tasks.
