# Machine Learning using RNNs and Transformers for Text Classification

In this project, I focused on applying machine learning concepts to build and train deep neural networks specifically designed for sequential data, including Recurrent Neural Networks (RNNs) and Transformers. The goal was to demonstrate a comprehensive understanding of these architectures by constructing models for a text classification task using a dataset of 2,000 questions categorised into six distinct classes: abbreviation, entity, description, human, location, and numeric.

The project involved several key components: preprocessing the data, implementing various RNN architectures (including simple RNNs, GRUs, and LSTMs), and utilizing Transformers for sequence modeling. I also explored hyperparameter tuning to optimise model performance. By employing techniques such as Word2Vec for text vectorisation and attention mechanisms within the RNNs, I aimed to enhance the models' ability to accurately classify questions based on their content.

Through this project, I gained practical experience in handling sequential data with deep learning models. I learned how to effectively implement RNNs and Transformers, understanding their strengths in capturing temporal dependencies in text. Additionally, I developed skills in model evaluation and optimisation, which are crucial for improving classification accuracy.
